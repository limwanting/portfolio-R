---
title: "Classification models"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

In this exercise, I will apply classification models (support vector machine and k nearest neighbour) to make predictions on successful credit card applications. The dataset is obtained from the “Credit Approval Data Set” from the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/datasets/Credit+Approval) without the categorical variables and without data points that have missing values.

The dataset has 654 data points, 6 continuous and 4 binary predictor variables. The last column is a binary response variable indicating if the application was positive or negative.

#### Loading datasets and libraries

First step is to load the dataset and check out the first few rows of the data. The last column R1 is the binary response variable.

```{r load data}
data <- read.table('credit_card_data-headers.txt',header=TRUE)
head(data)
```

Loading libraries that will be used.

```{r}
library(kernlab)
library(kknn)
```

### Classification Model 1 - Support Vector Machine

In this part, we will use the support vector machine function (ksvm) contained in the R package kernlab to find a good classifier for this data. We will also try other non-linear kernels besides vanilladot to try to find one that may provide better predictions.

#### Fitting our SVM classifier:

Fitting our SVM classifier to the credit card dataset using ksvm. ksvm requires the inputs to be a data matrix and factor, so the as.matrix and as.factor functions need to be applied.

Here, I will try three different kernel functions, namely vanilladot, rbfdot, and polydot. The performance of each model will be compared.

```{r}
set.seed(1)

# vanilladot
ksvm_model_vanilladot <- ksvm(x=as.matrix(data[,1:10]),as.factor(data[,11]), type = 'C-svc',kernel = 'vanilladot',C = 100,scaled = TRUE)
ksvm_model_vanilladot

# polydot
ksvm_model_polydot <- ksvm(x=as.matrix(data[,1:10]),as.factor(data[,11]), type = 'C-svc',kernel = 'polydot',C = 100,scaled = TRUE)
ksvm_model_polydot

# rbfdot
ksvm_model_rbfdot <- ksvm(x=as.matrix(data[,1:10]),as.factor(data[,11]), type = 'C-svc',kernel = 'rbfdot',C = 100,scaled = TRUE)
ksvm_model_rbfdot

```

After running the above, it is found that using rbfdot kernel returns the lowest training error.

I will generate predictions of each model using the predict function, and check what fraction of each model's predictions match the actual classification.

```{r}
preds_vanilladot <- predict(ksvm_model_vanilladot,data[,1:10])
preds_polydot <- predict(ksvm_model_polydot,data[,1:10])
preds_rbfdot <- predict(ksvm_model_rbfdot,data[,1:10])

# check what fraction of the model's predictions match the actual classification
sum(preds_vanilladot==data[,11]) / nrow(data)
sum(preds_polydot==data[,11]) / nrow(data)
sum(preds_rbfdot==data[,11]) / nrow(data)

```

Using rbfdot kernel also returns the highest accuracy as 95% of the predictions match the actual classification.

We shall use the rbfdot model as the chosen kernel.

Let's now experiment with different values of C:

```{r}
j <- 0
C_value <- c(0.01,0.1,1,10,100,1000,10000)
C_results <- rep(0,7)

for (i in C_value){
  j<-j+1
  ksvm_model <- ksvm(x=as.matrix(data[,1:10]),as.factor(data[,11]), type = 'C-svc',kernel = 'rbfdot',C = i,scaled = TRUE)
  # check what fraction of the model's predictions match the actual classification
  preds <- predict(ksvm_model,data[,1:10])
  accuracy <- sum(preds==data[,11]) / nrow(data)
  C_results[j] <- accuracy
}

data.frame(C_value,C_results)
```

Based on the results, I would choose C=100 as it returns about 95% accuracy on the dataset. C=1000 and C=10000 provides very high accuracy of >98% but the danger is overfitting to this model, as it is a nonlinear kernel.

Now, let's work out the equation for the classifier. As the ksvm model does not directly return the coefficients, use xmatrix and coef (attributes of the model) to calculate the coefficients.

The goal is to obtain the weight of each of the 10 attributes from our data, so that the classifier will be a1x1 + a2x2...+a0 = 0

```{r}
ksvm_model <- ksvm(x=as.matrix(data[,1:10]),as.factor(data[,11]), type = 'C-svc',kernel = 'rbfdot',C = 100,scaled = TRUE)
# calculate a1...am
a <- colSums(ksvm_model@xmatrix[[1]] * ksvm_model@coef[[1]])
# calculate a0
a0 <- -ksvm_model@b
a
a0
```

Based on the above, the equation will be (`r round(a[1],2)`)A1 + (`r round(a[2],2)`)A2 + (`r round(a[3],2)`)A3 + (`r round(a[4],2)`)A8 + (`r round(a[5],2)`)A9 + (`r round(a[6],2)`)A10 + (`r round(a[7],2)`)A11 + (`r round(a[8],2)`)A12 + (`r round(a[9],2)`)A14 + (`r round(a[10],2)`)A15 + `r round(a0,2)` = 0  

### Classification Model 2 - K nearest neighbours

The next part is to use kknn as the classification model. This is different from SVM as the methodology for KNN is to determine the k nearest neighbours to the target instance. In this context, k refers to the number of nearest neighbours to use in the model.

#### Loading relevant libraries

```{r}
library(kknn)
```

### Fitting KNN:

To know the accuracy for each value of k, the knn model needs to be created for each data point sequentially. At the end, we can then record the accuracy when comparing with the actual classification.

```{r}
set.seed(1)

predict <- rep(0,nrow(data))
accuracy <- rep(0,30)

for (kval in 1:30){

for (i in 1:nrow(data)){
  knn_model <- kknn(R1~.,data[-i,],data[i,],k=kval,scale=TRUE)
  # R1~. is a formula object, meaning to use all other variables than 'R1' as predictors to predict R1
  predict[i] <- floor(predict(knn_model)+0.5)
}
  accuracy[kval] <- sum(predict == data[,11]) / nrow(data)

}
```

Using the plot of accuracy against k value, we can see which value of k gives the highest accuracy.

Based on the plot below, we can see that k=12 and k=15 gives the highest accuracy.

```{r}
plot(accuracy,xlab = 'value of k')
```

```{r}
# k value at which accuracy is highest
kval <- which.max(accuracy)
maxaccurate <- round(max(accuracy)*100 , digits=2)
```

We can use k = `r kval` and it classifies the data points in the full dataset to `r maxaccurate`% accuracy.


### Using cross-validation for the knn model

To improve the model, we can also use train.knn to perform leave-one-out crossvalidation. First run some code to randomly select one third of the dataset to be the test set.

```{r}
sample_size <- floor(nrow(data)/3)
sample_set <- sample(1:nrow(data), sample_size, replace = FALSE, prob = NULL)

# selecting the non sample data points as the train set
train_set <- data[-sample_set,]

# selecting the sample data points as the test set
test_set <- data[sample_set,]

```

Next, let's train using train.kknn to find the optimal value of k.

```{r}

knn_model_train <- train.kknn(R1~., data = train_set, kmax = 50, kernel=c("optimal","rectangular", "inv", "gaussian", "triangular"), scale = TRUE) 

knn_model_train

```

Let's check the performance on the test set: 

```{r}
predict_frac <- predict(knn_model_train,test_set)

# convert the fraction to 0 or 1
predict_test <- floor(predict_frac + 0.5)

predict_accuracy <- sum(predict_test == test_set[,11]) / nrow(test_set)

predict_accuracy <- round(predict_accuracy*100,digit=2)

```

The predicted accuracy is `r predict_accuracy`%.

We can also have more details on the predictions by using a confusion matrix.

```{r}
table(predict_test,test_set[,11])

```


### Splitting the data into training, validation, and test sets

Another approach is to split the data into 3 sets - training, validation, and test data sets. For this particular dataset, I will not randomly split but use the rotation method (i.e. sequentially allocate each data point into train validation or test set).

This is because when observing the dataset earlier, the data was organised such that the first part of the dataset had response variable of 1 and the second part had response variable of 0. Due to this grouping, we probably should try the rotation method to prevent over-representing either response in our split datasets.

Training set - build the model
Validation set - picking a model
Test set - estimate performance of model

```{r}
validation <- seq(2,nrow(data),5)
validation_set <- data[validation,]

test <- seq(4,nrow(data),5)
test_set <- data[test,]

train1 <- seq(1,nrow(data),5)
train3 <- seq(3,nrow(data),5)
train5 <- seq(5,nrow(data),5)
train <- c(train1,train3,train5)
train_set <- data[train,]

```

I will train on both SVM and KNN models.

#### SVM model

```{r}
set.seed(1)
  ksvm_model_part3 <- ksvm(x=as.matrix(train_set[,1:10]),as.factor(train_set[,11]), type = 'C-svc',kernel = 'rbfdot',C = 100,scaled = TRUE)

preds_ksvm_train <- predict(ksvm_model_part3,train_set[,1:10])
accuracy_ksvm_train <- sum(preds_ksvm_train==train_set[,11]) / nrow(train_set)
```

The accuracy on the training set is 96%.

```{r}
preds_ksvm_val <- predict(ksvm_model_part3,validation_set[,1:10])
accuracy_ksvm_val <- sum(preds_ksvm_val==validation_set[,11]) / nrow(validation_set)
```

The accuracy on the validation set is 82%. The lowered accuracy is expected behaviour as this is a set of data that the model has yet to "see".

#### KNN model
```{r}
set.seed(1)
knn_model_part3 <- kknn(R1~.,train_set,validation_set,k=12,scale=TRUE)

predicted_knn_part3 <- floor(predict(knn_model_part3)+0.5)
accuracy_knn_part3 <- sum(predicted_knn_part3 == validation_set[,11]) / nrow(validation_set)
```

For the KNN model, after using the training set to train the model, the accuracy of the validation set is 84%.

Therefore, accuracy of validation set:
KSVM model: 82%
KNN model: 84%

We will select the KNN model as it performs better on the validation set. For the final evaluation of the effectiveness, the test set accuracy will be used.

```{r}
final_model_part3 <- kknn(R1~.,train_set,test_set,k=12,scale=TRUE)

final_pred_part3 <- floor(predict(final_model_part3)+0.5)
accuracy_part3 <- sum(final_pred_part3 == test_set[,11]) / nrow(test_set)
```

The performance of the KNN model on the test set is 82%. Therefore, we will judge the effectiveness of the selected model as 82%.
