---
title: "Credit card dataset"
knit: (function(input_file, encoding) {
  out_dir <- 'docs';
  rmarkdown::render(input_file,
 encoding=encoding,
 output_file=file.path(dirname(input_file), out_dir, 'index.html'))})
author: "Wan Ting"
date: "22/08/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

In this file, I will be doing an exploratory analysis using a sample dataset.

#### Loading datasets

First step is to load the relevant dataset.

```{r load data}
data <- read.table('credit_card_data-headers.txt',header=TRUE)
summary(data)
```

#### Loading libraries needed

Next we will need to load the relevant libraries.

```{r}
library(kernlab)
library(kknn)
```

### Part 1 and 2

In this part, we will use the support vector machine function (ksvm) contained in the R package kernlab to find a good classifier for this data. We will also try other non-linear kernels besides vanilladot to try to find one that may provide better predictions.

#### Fitting our SVM classifier:

Fitting our SVM classifier to the credit card dataset using ksvm. ksvm requires the inputs to be a data matrix and factor, so the as.matrix and as.factor functions need to be applied.

Here, I will try three different kernel functions, namely vanilladot, rbfdot, and polydot. The performance of each model will be compared.

```{r}
# vanilladot
ksvm_model_vanilladot <- ksvm(x=as.matrix(data[,1:10]),as.factor(data[,11]), type = 'C-svc',kernel = 'vanilladot',C = 100,scaled = TRUE)
ksvm_model_vanilladot

# polydot
ksvm_model_polydot <- ksvm(x=as.matrix(data[,1:10]),as.factor(data[,11]), type = 'C-svc',kernel = 'polydot',C = 100,scaled = TRUE)
ksvm_model_polydot

# rbfdot
ksvm_model_rbfdot <- ksvm(x=as.matrix(data[,1:10]),as.factor(data[,11]), type = 'C-svc',kernel = 'rbfdot',C = 100,scaled = TRUE)
ksvm_model_rbfdot

```

After running the above, it is found that using rbfdot kernel returns the lowest training error.

I will generate predictions of each model using the predict function, and check what fraction of each model's predictions match the actual classification.

```{r}
preds_vanilladot <- predict(ksvm_model_vanilladot,data[,1:10])
preds_polydot <- predict(ksvm_model_polydot,data[,1:10])
preds_rbfdot <- predict(ksvm_model_rbfdot,data[,1:10])

# check what fraction of the model's predictions match the actual classification
sum(preds_vanilladot==data[,11]) / nrow(data)
sum(preds_polydot==data[,11]) / nrow(data)
sum(preds_rbfdot==data[,11]) / nrow(data)

```

Using rbfdot kernel also returns the highest accuracy as 95% of the predictions match the actual classification.

We shall use the rbfdot model as the chosen kernel.

Let's now experiment with different values of C:

```{r}
j <- 0
C_results <- rep(0,7)

for (i in c(0.01,0.1,1,10,100,1000,10000)){
  j<-j+1
  ksvm_model <- ksvm(x=as.matrix(data[,1:10]),as.factor(data[,11]), type = 'C-svc',kernel = 'rbfdot',C = i,scaled = TRUE)
  # check what fraction of the model's predictions match the actual classification
  preds <- predict(ksvm_model,data[,1:10])
  accuracy <- sum(preds==data[,11]) / nrow(data)
  C_results[j] <- accuracy
}

C_results
```

Based on the results, I would choose C=100 as it returns about 94% accuracy on the dataset. From C=0.1 onwards, the accuracy is quite commendable. C=1000 and C=10000 provides very high accuracy of >98% but the danger is overfitting to this model, as it is a nonlinear kernel.

Now, I can work out the equation for the classifier. As the ksvm model does not directly return the coefficients, use xmatrix and coef (attributes of the model) to calculate the coefficients.

The goal is to obtain the weight of each of the 10 attributes from our data, so that the classifier will be a1x1 + a2x2...+a0 = 0

```{r}
ksvm_model <- ksvm(x=as.matrix(data[,1:10]),as.factor(data[,11]), type = 'C-svc',kernel = 'rbfdot',C = 100,scaled = TRUE)
# calculate a1...am
a <- colSums(ksvm_model@xmatrix[[1]] * ksvm_model@coef[[1]])
# calculate a0
a0 <- -ksvm_model@b
a
a0
```

Based on the above, the equation will be (`r a[1]`)A1 + (`r a[2]`)A2 + (`r a[3]`)A3 + (`r a[4]`)A8 + (`r a[5]`)A9 + (`r a[6]`)A10 + (`r a[7]`)A11 + (`r a[8]`)A12 + (`r a[9]`)A14 + (`r a[10]`)A15 + `r a0` = 0

### Part 3 - Using kknn to suggest a good value of k

The next part is to use kknn. This is different from SVM as the methodology for KNN is to determine the k nearest neighbours to the target instance. In this context, k refers to the number of nearest neighbours to use in the model.

#### Loading relevant libraries

```{r}
library(kknn)
```

### Fitting KNN:

To know the accuracy for each value of k, the knn model needs to be created for each data point sequentially. At the end, we can then record the accuracy when comparing with the actual classification.

```{r}

predict <- rep(0,nrow(data))
accuracy <- rep(0,30)

for (kval in 1:30){

for (i in 1:nrow(data)){
  knn_model <- kknn(R1~.,data[-i,],data[i,],k=kval,scale=TRUE)
  # R1~. is a formula object, meaning to use all other variables than 'R1' as predictors to predict R1
  predict[i] <- floor(predict(knn_model)+0.5)
}
  accuracy[kval] <- sum(predict == data[,11]) / nrow(data)

}
```

Using the plot of accuracy against k value, we can see which value of k gives the highest accuracy.

Based on the plot below, we can see that k=12 and k=15 gives the highest accuracy.

```{r}
plot(accuracy,xlab = 'value of k')
```

```{r}
# k value at which accuracy is highest
kval <- which.max(accuracy)
maxaccurate <- round(max(accuracy)*100 , digits=2)
```

We can use k = `r kval` and it classifies the data points in the full dataset to `r maxaccurate`% accuracy.

## Question 3.1

### Part a - Using cross-validation for the knn model

In this section, we will use train.knn to perform leave-one-out crossvalidation. Randomly select one third of the dataset to be the test set.

```{r}
sample_size <- floor(nrow(data)/3)
sample_set <- sample(1:nrow(data), sample_size, replace = FALSE, prob = NULL)

# selecting the non sample data points as the train set
train_set <- data[-sample_set,]

# selecting the sample data points as the test set
test_set <- data[sample_set,]

```

Next, let's train using train.kknn to find the optimal value of k.

```{r}

knn_model_train <- train.kknn(R1~., data = train_set, kmax = 50, kernel=c("optimal","rectangular", "inv", "gaussian", "triangular"), scale = TRUE) 

knn_model_train

```

Let's check the performance on the test set: 

```{r}
predict_frac <- predict(knn_model_train,test_set)

# convert the fraction to 0 or 1
predict_test <- floor(predict_frac + 0.5)

predict_accuracy <- sum(predict_test == test_set[,11]) / nrow(test_set)

predict_accuracy <- round(predict_accuracy*100,digit=2)

```

The predicted accuracy is `r predict_accuracy`%.

We can also have more details on the predictions by using a confusion matrix.

```{r}
table(predict_test,test_set[,11])

```


### Part b - Splitting the data into training, validation, and test sets

I will now split the data into training, validation, and test data sets. I will be using the rotation method. This is because the dataset sequence appears to have the 0s and 1s clumped together, so we don't want to over-represent either response in our split datasets.

Training set - build the model
Validation set - picking a model
Test set - estimate performance of model

```{r}
validation <- seq(2,nrow(data),5)
validation_set <- data[validation,]

test <- seq(4,nrow(data),5)
test_set <- data[test,]

train1 <- seq(1,nrow(data),5)
train3 <- seq(3,nrow(data),5)
train5 <- seq(5,nrow(data),5)
train <- c(train1,train3,train5)
train_set <- data[train,]

```

I will train on both SVM and KNN models.

#### SVM model

```{r}
  ksvm_model_part3 <- ksvm(x=as.matrix(train_set[,1:10]),as.factor(train_set[,11]), type = 'C-svc',kernel = 'rbfdot',C = 100,scaled = TRUE)

preds_ksvm_train <- predict(ksvm_model_part3,train_set[,1:10])
accuracy_ksvm_train <- sum(preds_ksvm_train==train_set[,11]) / nrow(train_set)
```

The accuracy on the training set is 96%.

```{r}
preds_ksvm_val <- predict(ksvm_model_part3,validation_set[,1:10])
accuracy_ksvm_val <- sum(preds_ksvm_val==validation_set[,11]) / nrow(validation_set)
```

The accuracy on the validation set is 82%. The lowered accuracy is expected behaviour as this is a set of data that the model has yet to "see".

#### KNN model
```{r}
knn_model_part3 <- kknn(R1~.,train_set,validation_set,k=12,scale=TRUE)

predicted_knn_part3 <- floor(predict(knn_model_part3)+0.5)
accuracy_knn_part3 <- sum(predicted_knn_part3 == validation_set[,11]) / nrow(validation_set)
```

For the KNN model, after using the training set to train the model, the accuracy of the validation set is 84%.

Therefore, accuracy of validation set:
KSVM model: 82%
KNN model: 84%

We will select the KNN model as it performs better on the validation set. For the final evaluation of the effectiveness, the test set accuracy will be used.

```{r}
final_model_part3 <- kknn(R1~.,train_set,test_set,k=12,scale=TRUE)

final_pred_part3 <- floor(predict(final_model_part3)+0.5)
accuracy_part3 <- sum(final_pred_part3 == test_set[,11]) / nrow(test_set)
```

The performance of the KNN model on the test set is 82%. Therefore, we will judge the effectiveness of the selected model as 82%.
